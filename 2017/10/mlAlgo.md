## Some popular Machine Learning Algorithms and related concepts

While learning different papers related to deep learning I often stumble across concepts that require ML algorithms that have been state-of-the-art for some specific problems. So I decided to make a curared list and it will be updated as an when I find them worth mentioning here. I also think the best way to learn these algorithms is by understanding and implementing through the code. Hence, I try to find/code the algorithms in Python/C without the use of Deep Learning frameworks.

### First and Second order optimization techniques

It is topic in itself how Gradient descent and its derivatives work to optimise an objective function. These approaches are mostly first order(SGD) and highly dependent upon the training model and dataset.

- #### Math behind optimization techniques <[video](https://www.youtube.com/watch?v=UIFMLK2nj_w)>

- #### Stochastic Gradient Descent

- #### RMSProp

- #### Adam

- #### Adagrad

- #### Nestrov Adam (Nadam)

### Backpropagation

### Classification Techniques

- #### Adaboost algorithms

- #### Boltzmann Machines

- #### Support Vector Machine <[code](https://gist.github.com/mblondel/586753), [video](https://www.youtube.com/watch?v=g8D5YL6cOSE)>

- #### Decision Tree

- #### Random Forest

### Feature Extraction Techniques in Computer Vision

- #### Sparse Coding

- #### Bag of Visual Words

- #### Transfer Learning in Deep Convolution Neural Networks

